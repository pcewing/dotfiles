#!/usr/bin/env python

# This script provides a fuzzy file finder that caches results for performance.
# In a normal Linux environment, this isn't really necessary but on WSL, where
# file I/O is miserably slow, it's pretty helpful.
#
# The tl;dr of what this does:
# - Checks if a cache file exists
#   - If it does, reads and prints the contents of the cache file to stdout
#     immediately and then pre-populates a set of enumerated files
#   - If not, initializes an empty set of enumerated files
# - Enumerates all files in the directory
#   - If the file is not in the set, prints it to stdout and adds it to the set
# - Writes the enumerated files to a new cache file
#   - To avoid naming collisions with other instances that may be running in
#     parallel, the cache files are named as follows:
#     - <cache_id>-<timestamp>-<random_token>.txt
#   - At the end of a successful run, the script will delete cache files that
#     already existed when it began
#     - This can be disabled via the `--no-tidy` flag
#
# Not that a drawback of this approach is that files that are deleted on disk
# may persist indefinitely in the cache; however, this is a tradeoff for
# performance. To manually clean up the cache, run the tool with the `--clean`
# flag.

import argparse
import glob
import hashlib
import os
import random
import re
import string
import sys
import time
from typing import Optional

DOTFILES_DIR = os.getenv("DOTFILES")
if DOTFILES_DIR is None:
    raise Exception("DOTFILES environment variable not specified")
sys.path.append(os.path.join(DOTFILES_DIR, "cli"))

from lib.common.dir import Dir
from lib.common.file_walker import FileWalker
from lib.common.log import Log
from lib.common.util import Util


def parse_args():
    parser = argparse.ArgumentParser(
        description="Fuzzy File Finder that caches results for performance"
    )
    parser.add_argument(
        "-l",
        "--log-level",
        default="info",
        help="Logging level to run with (debug, info, warn, error, crit)",
    )
    parser.add_argument(
        "--no-cache",
        dest="cache",
        action="store_false",
        help="Don't use cached results",
    )
    parser.add_argument(
        "--directory",
        default=os.getcwd(),
        help="Directory to find files in",
    )
    parser.add_argument(
        "--no-tidy",
        dest="tidy",
        action="store_false",
        help="Disable the tidying mechanism that removes old cache files",
    )
    parser.add_argument(
        "-C",
        "--clean",
        action="store_true",
        help="Delete all existing cache and PID files without running",
    )
    return parser.parse_args()


class FuzzyFileFinder:
    @staticmethod
    def find_files(directory, on_file):
        # TODO: It would be cool to support .fzfignore files in sub-directories
        # but that gets a little more complicated because we'd need to layer
        # them on top of each other while recursively descending and then
        # remove them as we traverse back up. Definitely possible but just
        # requires more effort than I want to put in right now.

        # TODO: I don't think these need to be separated, we can probably just
        # have a single list of ignore patterns that match on either files or
        # directories. There may be times where we want a pattern to only match
        # against one or the other but I can't think of any specific examples
        # right now. It is kind of nice in the .fzfignore file to see which
        # patterns are expected to match against directories though. Anyways,
        # leaving them separate for now but can revisit later.
        file_ignore_patterns = []
        dir_ignore_patterns = []
        ignore_file = os.path.join(directory, ".fzfignore")
        if os.path.isfile(ignore_file):
            ignore_file_content = None
            with open(ignore_file, "r") as f:
                ignore_file_content = f.read()
            lines = list(
                filter(
                    lambda line: len(line) > 0,
                    [line.strip() for line in ignore_file_content.split("\n")],
                )
            )
            for line in lines:
                if line.lstrip().startswith("#") or line.strip() == "":
                    continue
                elif line.lower().startswith("d "):
                    dir_ignore_patterns.append(line[2:])
                else:
                    file_ignore_patterns.append(line)

        def handle_dir(
            directory: FileWalker.Directory,
        ) -> Optional[FileWalker.DirectoryHandlerResult]:
            name = directory.get_name()
            if name == ".git":
                return FileWalker.DirectoryHandlerResult(skip=True)
            for ignore_pattern in dir_ignore_patterns:
                path_rel = directory.get_relative_path()
                m = re.match(ignore_pattern, path_rel)
                if m is not None:
                    # Keep this commented out for performance except when debugging
                    # Log.debug("ignoring directory", {"ignore_pattern": ignore_pattern, "dir": path_rel})
                    return FileWalker.DirectoryHandlerResult(skip=True)

        def handle_file(
            file: FileWalker.File,
        ) -> Optional[FileWalker.FileHandlerResult]:
            name = file.get_name()
            for ignore_pattern in file_ignore_patterns:
                path_rel = file.get_relative_path()
                m = re.match(ignore_pattern, path_rel)
                if m is not None:
                    # Keep this commented out for performance except when debugging
                    # Log.debug("ignoring file", {"ignore_pattern": ignore_pattern, "file": path_rel})
                    return

            on_file(file.get_relative_path())

        FileWalker.walk(
            directory,
            file_handler=handle_file,
            directory_handler=handle_dir,
        )


class Cache:
    def __init__(
        self, tmp_directory: str, directory: str, use_existing: bool, tidy: bool
    ) -> None:
        self._tmp_directory = tmp_directory
        self._directory = directory
        self._use_existing = use_existing
        self._should_tidy = tidy

        self._cache_id = Cache._get_cache_id(directory)
        Log.info("cache id determined", {"cache_id": self._cache_id})

        self._existing_cache_files = self._get_existing_cache_files(
            self._tmp_directory, self._cache_id
        )
        Log.info(
            "scanned existing cache files",
            [
                ("count", len(self._existing_cache_files)),
                ("files", self._existing_cache_files),
            ],
        )

        self._cached_files = set()

    def update(self) -> None:
        self._cached_files = self._load()
        self._enumerate()
        self._update()
        if self._should_tidy:
            self._tidy()

    def _enumerate(self) -> None:
        Log.info("enumerating and printing files")
        FuzzyFileFinder.find_files(self._directory, self._on_file)

    def _on_file(self, relative_path):
        # Only add/print files we don't already know about
        if relative_path not in self._cached_files:
            print(relative_path)
            self._cached_files.add(relative_path)

    def _update(self) -> None:
        cache_content = "\n".join(self._cached_files) + "\n"

        timestamp = time.monotonic_ns()
        random_token = "".join(
            random.choices(string.ascii_lowercase + string.digits, k=8)
        )

        cache_file_name = f"{self._cache_id}-{timestamp}-{random_token}.txt"
        cache_file_path = os.path.join(self._tmp_directory, cache_file_name)

        with open(cache_file_path, "w") as f:
            f.write(cache_content)

    def _tidy(self) -> None:
        for cache_file in self._existing_cache_files:
            Log.info("removing old cache file", {"cache_file": cache_file})
            try:
                os.remove(cache_file)
            except FileNotFoundError:
                pass

    # If caching is enabled and the cache file exists, load the cache from that
    # file, print its contents to stdout, and return the files in the cache. If
    # caching is disabled or the cache file does not exist, this will early out and
    # return None.
    def _load(self) -> set[str]:
        if not self._use_existing:
            Log.info("skipping cache load", {"reason": "--no-cache flag specified"})
            return set()

        if len(self._existing_cache_files) == 0:
            Log.info("skipping cache load", {"reason": "cache file does not exist"})
            return set()

        cache_file = self._existing_cache_files[0]
        Log.info("loading cache from file", {"cache_file": cache_file})

        cache_file_content = None
        with open(cache_file, "r") as f:
            cache_file_content = f.read()

        Log.info("printing existing cache contents to stdout")
        print(cache_file_content)

        return Cache._parse_cached_files(cache_file_content)

    @staticmethod
    def _get_existing_cache_files(tmp_dir: str, cache_id: str) -> list[str]:
        existing_cache_files = glob.glob(f"{tmp_dir}/{cache_id}-*.txt")

        def get_file_timestamp(file: str) -> int:
            return int(file.split("-")[1])

        # Sort the files by timestamp descending
        existing_cache_files.sort(key=lambda f: get_file_timestamp(f), reverse=True)
        return existing_cache_files

    @staticmethod
    def _get_cache_id(directory: str) -> str:
        return hashlib.sha1(directory.encode("utf-8")).hexdigest()

    @staticmethod
    def _parse_cached_files(cache_file_content: str) -> set[str]:
        lines = [l.strip() for l in cache_file_content.split("\n")]
        return set([l for l in lines if l != ""])


def clean(tmp_dir: str):
    try:
        Log.info("removing existing tmp directory", {"tmp_dir": tmp_dir})
        Util.rmdir(tmp_dir)
    except FileNotFoundError:
        pass


def main():
    args = parse_args()
    log_level = Log.parse_level(args.log_level)

    tmp_dir = os.path.join(Dir.home(), ".tmp", "fzf_cached_wsl")
    os.makedirs(tmp_dir, exist_ok=True)

    # Log to stdout if we're just cleaning up files, otherwise log to a file so
    # we can print the fuzzy finder results to stdout
    if args.clean:
        Log.init("fzf_cached_wsl", log_level)
        clean(tmp_dir)
        return

    log_file = os.path.join(tmp_dir, "log.txt")
    Log.init("fzf_cached_wsl", log_level, stdout=False, file=log_file)

    Log.info(
        "fzf_cached_wsl started", [("cache", args.cache), ("directory", args.directory)]
    )

    cache = Cache(tmp_dir, args.directory, args.cache, args.tidy)
    cache.update()

    Log.info("fzf_cached_wsl finished")


if __name__ == "__main__":
    main()
