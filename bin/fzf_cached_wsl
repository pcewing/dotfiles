#!/usr/bin/env python

import argparse
import fcntl
import hashlib
import os
import re
import random
import shutil
import signal
import string
import sys
import threading

from typing import Optional

DOTFILES_DIR = os.getenv("DOTFILES")
if DOTFILES_DIR is None:
    raise Exception("DOTFILES environment variable not specified")
sys.path.append(os.path.join(DOTFILES_DIR, "cli"))

from lib.common.log import Log
from lib.common.file_walker import FileWalker
from lib.common.util import Util

PID_FILE = None


def signal_handler(sig, frame):
    PID_FILE.delete()

    default_handler = None
    if sig == signal.SIGTERM:
        default_handler = signal.default_term_handler
    elif sig == signal.SIGINT:
        default_handler = signal.default_int_handler
    if default_handler is not None:
        default_handler(sig, frame)


def parse_args():
    parser = argparse.ArgumentParser(
        description="Fuzzy File Finder that caches results for performance"
    )
    parser.add_argument(
        "-l",
        "--log-level",
        default="info",
        help="Logging level to run with (debug, info, warn, error, crit)",
    )
    parser.add_argument(
        "--no-cache",
        dest="cache",
        action="store_false",
        help="Don't use cached results",
    )
    parser.add_argument(
        "--directory",
        default=os.getcwd(),
        help="Directory to find files in",
    )
    parser.add_argument(
        "-C",
        "--clean",
        action="store_true",
        help="Delete all existing cache and PID files without running",
    )
    return parser.parse_args()


class FuzzyFileFinder:
    @staticmethod
    def find_files(directory, on_file):
        ignore_patterns = []
        ignore_file = os.path.join(directory, ".fzfignore")
        if os.path.isfile(ignore_file):
            ignore_file_content = None
            with open(ignore_file, "r") as f:
                ignore_file_content = f.read()
            lines = list(
                filter(
                    lambda line: len(line) > 0,
                    [line.strip() for line in ignore_file_content.split("\n")],
                )
            )
            for line in lines:
                if line.startswith("#"):
                    continue
                ignore_patterns.append(line)

        def handle_dir(
            directory: FileWalker.Directory,
        ) -> Optional[FileWalker.DirectoryHandlerResult]:
            if directory.get_name() == ".git":
                return FileWalker.DirectoryHandlerResult(skip=True)

        def handle_file(
            file: FileWalker.File,
        ) -> Optional[FileWalker.FileHandlerResult]:
            name = file.get_name()
            for ignore_pattern in ignore_patterns:
                m = re.match(ignore_pattern, file.get_relative_path())
                if m is not None:
                    # Keep this commented out for performance except when debugging
                    # Log.debug("ignoring file", [("ignore_pattern", ignore_pattern), ("file", file.get_relative_path())])
                    return

            on_file(file.get_relative_path())

        FileWalker.walk(
            directory,
            file_handler=handle_file,
            directory_handler=handle_dir,
        )


def update_cache(directory: str, cache_file: str, quiet: bool):
    random_string = "".join(random.choices(string.ascii_lowercase + string.digits, k=8))
    tmpfile = f"/tmp/{random_string}.txt"
    with open(tmpfile, "w") as f:

        def on_file(relative_path):
            if not quiet:
                print(relative_path)
            f.write(relative_path + "\n")

        FuzzyFileFinder.find_files(directory, on_file)
    shutil.move(tmpfile, cache_file)


def run_async(directory: str, cache_file: str):
    Log.info("reading cached results")
    results = None
    with open(cache_file, "r") as f:
        results = f.read()

    Log.info("initiating asynchronous cache update")
    thread = threading.Thread(target=update_cache, args=(directory, cache_file, True))
    thread.start()

    Log.info("printing cached results")
    print(results)

    Log.info("waiting for asynchronous cache update to complete")
    thread.join()


def run_sync(directory: str, cache_file: str):
    Log.info("synchronously refreshing cache and printing results")
    update_cache(directory, cache_file, False)


def already_running(tmp_dir: str):
    return PID_FILE.exists(ignore_self=True)


def run(args, tmp_dir: str):
    Log.info("fzf_cached_wsl started", [("cache", args.cache), ("directory", args.directory)])

    dir_hash = hashlib.sha1(args.directory.encode("utf-8")).hexdigest()
    cache_file = os.path.join(tmp_dir, f"{dir_hash}.txt")

    Log.info("cache file path resolved", [("cache_file", cache_file)])

    cache_exists = os.path.isfile(cache_file)

    if args.cache and cache_exists and already_running(tmp_dir):
        # TODO: How do we handle this? On one hand, it would be nice to just
        # stop and let the other process finish but we need to print something.
        # If there's no existing cache then we have nothing to print if we
        # stop. On the other hand it would be nice if the "newest" process ran
        # and killed the older processes so that the resulting cache is more
        # up-to-date with any file changes that may have occurred in between.
        #
        # Also, if we just kill another running instance but that instance was
        # synchronously updating, we are going to halt any further results it
        # may have shown.
        #
        # Maybe each process just writes a `xxxx.pid` file when it starts and
        # we can see if any exist. If one exists and the cache also already
        # exists, just print the cache and let that other process finish and
        # update the cache. If one exists and the cache doesn't exist, let both
        # processes run and race to update the cache? We could also add a third
        # mode where we print results but don't update the cache so the two
        # don't race but then a third instance wouldn't know what "mode" the
        # other instances are running in so that would further complicate
        # things.
        #
        # I think the simplest optimization that will cover most cases is:
        # - If another instance is already running and the cache file exists,
        #   print the cache and exit without updating it again
        Log.info("another instance already running, printing cache")
        with open(cache_file, "r") as f:
            print(f.read())
            return

    Log.info(
        "initializing cache file", [("path", cache_file), ("exists", cache_exists)]
    )

    if args.cache and cache_exists:
        run_async(args.directory, cache_file)
    else:
        run_sync(args.directory, cache_file)

    Log.info("fzf_cached_wsl finished")


# TODO: Add a cleanup mechanism for when these do get orphaned due to process
# being abnormally terminated or something. Maybe check the creation time on
# the PID files and just nuke any that are obviously not still running.
class PidFile:
    def __init__(self, directory, prefix):
        self._dir = directory
        self._prefix = prefix

        self._pid = str(os.getpid())
        self._name = f"{self._prefix}-{self._pid}.pid"
        self._path = os.path.join(self._dir, self._name)

    def create(self):
        with open(self._path, "w") as f:
            f.write(self._pid)

    def delete(self):
        try:
            os.remove(self._path)
        except FileNotFoundError:
            pass

    def exists(self, ignore_self=False):
        files = os.listdir(self._dir)
        for file in files:
            if file == self._name:
                if ignore_self:
                    continue
                else:
                    return True
            if file.startswith(self._prefix):
                return True
        return False

def clean(tmp_dir:str):
    try:
        Log.info("removing existing tmp directory", [("tmp_dir", tmp_dir)])
        Util.rmdir(tmp_dir)
    except FileNotFoundError:
        pass

def main():
    args = parse_args()
    log_level = Log.parse_level(args.log_level)

    tmp_dir = os.path.join(os.getenv("HOME"), ".tmp/fzf_cached_wsl")
    os.makedirs(tmp_dir, exist_ok=True)

    if args.clean:
        Log.init("fzf_cached_wsl", log_level)
        clean(tmp_dir)
        return

    log_file = os.path.join(tmp_dir, "log.txt")
    Log.init("fzf_cached_wsl", log_level, stdout=False, file=log_file)

    pid_file = PidFile(tmp_dir, "fzf-cached-wsl")
    pid_file.create()

    global PID_FILE
    PID_FILE = pid_file
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

    try:
        run(args, tmp_dir)
    finally:
        pid_file.delete()


if __name__ == "__main__":
    main()
